{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpZ42Un36nt89ko6AKys7T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuliaKulacz/Fake-News-Detection/blob/main/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYj4PvKQgdV5",
        "outputId": "48e3200d-9972-480d-8e9e-1991553fcc49"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__PpXlNLgrwy",
        "outputId": "ca842b66-2f66-4ed8-adc4-10b2607622f3"
      },
      "source": [
        "! pip install torch"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH84Hmltg1Lo",
        "outputId": "337274d2-070e-4039-98b3-b30494876cc5"
      },
      "source": [
        "! pip install pytorch-pretrained-bert"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.18.65)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.5.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.65 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.21.65)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.65->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.65->boto3->pytorch-pretrained-bert) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.65->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQN8Q8nvyG-v",
        "outputId": "729aebc9-25c4-45d6-d405-d39a6e6bdd97"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di0_0dkKhDln",
        "outputId": "a7d71daa-1d01-4dbc-e055-aecade8b8e42"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")\n",
        "print(device)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYovzZxFhING"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import gensim\n",
        "import re\n",
        "import html\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ngrams\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import feature_extraction, model_selection, manifold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import precision_recall_curve, accuracy_score, confusion_matrix, roc_auc_score, classification_report, roc_curve, auc\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, AdamW\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-LERXAPhZGz"
      },
      "source": [
        "#DATA\n",
        "\n",
        "df_train = pd.read_csv(\"/content/data/train.csv\")\n",
        "df_test = pd.read_csv(\"/content/data/test.csv\")\n",
        "df_labels = pd.read_csv(\"/content/data/submit.csv\")\n",
        "\n",
        "df_test['label'] = df_labels['label']\n",
        "\n",
        "df = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
        "\n",
        "df.drop(columns=['author'], inplace=True)\n",
        "\n",
        "# Combining 'title' and 'text' colmuns together\n",
        "df['original'] = df['title'] + ' ' + df['text']\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "#Dropping duplicates\n",
        "df = df.drop_duplicates(subset=['text'])"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVesGCnqjHGo"
      },
      "source": [
        "#DATA CLEANING\n",
        "\n",
        "# Obtaining additional stopwords from nltk\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def cleaning(text):\n",
        "    clean = re.sub('<.*?>', ' ', str(text))         \n",
        "#removes HTML tags\n",
        "    clean = re.sub('\\'.*?\\s',' ', clean)               \n",
        "#removes all hanging letters afer apostrophes (s in it's)\n",
        "    clean = re.sub(r'http\\S+',' ', clean)              \n",
        "#removes URLs\n",
        "    clean = re.sub('\\W+',' ', clean)                   \n",
        "#replacing the non alphanumeric characters\n",
        "    return html.unescape(clean)\n",
        "df['cleaned'] = df['original'].apply(cleaning)\n",
        "\n",
        "\n",
        "def stopwords(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        # Taking words that don't belong to stopwords and have more than 2 characters\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
        "            result.append(token)\n",
        "\n",
        "    return result\n",
        "df['nostopwords'] = df['cleaned'].apply(stopwords)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkf33ZUku9Yq"
      },
      "source": [
        "# Lemmatizing\n",
        "wnl = WordNetLemmatizer()\n",
        "df['clean_lemm'] = df['nostopwords'].apply(lambda x: [wnl.lemmatize(word, pos=\"v\") for word in x])\n",
        "df['clean_lemm'] = df['clean_lemm'].apply(lambda x: [wnl.lemmatize(word, pos=\"a\") for word in x])\n",
        "df['clean_lemm'] = df['clean_lemm'].apply(lambda x: [wnl.lemmatize(word, pos=\"n\") for word in x])"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BokTAan4u_Kd",
        "outputId": "e1d3dbe8-1dbc-4fc8-f821-5be46ac28e82"
      },
      "source": [
        "# All unique words present in dataset in one string\n",
        "df['clean_joined'] = df['clean_lemm'].apply(lambda x: \" \".join(x))\n",
        "print(df['clean_joined'])"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19324    trump campaign admit try steal election voter ...\n",
            "573                                                       \n",
            "22077    dennis rodman north korea trip try open door b...\n",
            "23485    funny anti trump campaign video funny anti tru...\n",
            "25757    thyroid cancer scare test remove thyroid cance...\n",
            "                               ...                        \n",
            "5306     lawmaker approve name officer shoot print city...\n",
            "6574     secret nazi base treasure hunter discover arct...\n",
            "1139     comment hillary clinton control agenda expose ...\n",
            "25853    world community fear hillary kill list world c...\n",
            "10582    yemeni year guantánamo freedom soul take long ...\n",
            "Name: clean_joined, Length: 25433, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVQ5EuU6gzFT"
      },
      "source": [
        "# import BERT-base pretrained model\n",
        "#bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRM3gZuci1zn"
      },
      "source": [
        "# Splitting train dataset into train, validation and test sets\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(df['clean_joined'], df['label'], random_state=2018, test_size=0.3, stratify=df['label'])\n",
        "\n",
        "\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, random_state=2018, test_size=0.5, stratify=y_temp)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfgVYyb8laOZ",
        "outputId": "932a00d3-ff96-47be-dcf2-965375bb9f65"
      },
      "source": [
        "# Tokenizing and encoding sequences\n",
        "tokens_train = tokenizer.batch_encode_plus( x_train.tolist(), max_length = 25, pad_to_max_length=True, truncation=True)\n",
        "tokens_val = tokenizer.batch_encode_plus( x_val.tolist(), max_length = 25, pad_to_max_length=True, truncation=True)\n",
        "tokens_test = tokenizer.batch_encode_plus( x_test.tolist(), max_length = 25, pad_to_max_length=True, truncation=True)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv2XLsYXnlKB"
      },
      "source": [
        "# Converting lists to tensors\n",
        "\n",
        "train_tokens_tensor = torch.tensor(tokens_train['input_ids'])\n",
        "train_masks_tensor = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y_tensor = torch.tensor(y_train.tolist())\n",
        "\n",
        "val_tokens_tensor = torch.tensor(tokens_val['input_ids'])\n",
        "val_masks_tensor = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y_tensor = torch.tensor(y_val.tolist())\n",
        "\n",
        "test_tokens_tensor = torch.tensor(tokens_test['input_ids'])\n",
        "test_masks_tensor = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y_tensor = torch.tensor(y_test.tolist())\n",
        "\n",
        "# Prepairing dataloaders\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_dataset = TensorDataset(val_tokens_tensor, val_masks_tensor, val_y_tensor)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3nzuqCapRWp"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-aOhEk0pXzG"
      },
      "source": [
        "#BUILDING AND TRAINING MODEL\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        ")\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4ycM8vprZ6X"
      },
      "source": [
        "# Defining the optimizer\n",
        "optimizer = AdamW(model.parameters(),lr = 1e-5)          # lr - learning rate"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSkLzBrbrxOd",
        "outputId": "2c4d4315-92e6-46ce-afe7-4058cd3cd46d"
      },
      "source": [
        "# Computing the class weights\n",
        "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "\n",
        "print(\"Class Weights:\",class_weights)\n",
        "\n",
        "# Converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# Pushing weights to GPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# Defining the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) "
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights: [1.00118097 0.99882181]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuTAhitbsIPV"
      },
      "source": [
        "# Function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # model predictions\n",
        "  total_preds=[]\n",
        "\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # push model predictions to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2wpS5X6sf0c"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6dnKncusrXL",
        "outputId": "e59e6d5f-d37e-4ae6-d22d-94025cf256ee"
      },
      "source": [
        "# number of training epochs\n",
        "epochs = 2\n",
        "\n",
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 2\n",
            "  Batch    50  of    557.\n",
            "  Batch   100  of    557.\n",
            "  Batch   150  of    557.\n",
            "  Batch   200  of    557.\n",
            "  Batch   250  of    557.\n",
            "  Batch   300  of    557.\n",
            "  Batch   350  of    557.\n",
            "  Batch   400  of    557.\n",
            "  Batch   450  of    557.\n",
            "  Batch   500  of    557.\n",
            "  Batch   550  of    557.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    120.\n",
            "  Batch   100  of    120.\n",
            "\n",
            "Training Loss: -7.986\n",
            "Validation Loss: -12.527\n",
            "\n",
            " Epoch 2 / 2\n",
            "  Batch    50  of    557.\n",
            "  Batch   100  of    557.\n",
            "  Batch   150  of    557.\n",
            "  Batch   200  of    557.\n",
            "  Batch   250  of    557.\n",
            "  Batch   300  of    557.\n",
            "  Batch   350  of    557.\n",
            "  Batch   400  of    557.\n",
            "  Batch   450  of    557.\n",
            "  Batch   500  of    557.\n",
            "  Batch   550  of    557.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    120.\n",
            "  Batch   100  of    120.\n",
            "\n",
            "Training Loss: -14.873\n",
            "Validation Loss: -17.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYJzwfG2tZVv"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))\n",
        "\n",
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_tokens_tensor.to(device), test_masks_tensor.to(device))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "\n",
        "preds = np.argmax(preds, axis = 1)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YOIVB9ot21sF",
        "outputId": "58ed76e7-cafc-4d5c-e4c5-b77893fb76d1"
      },
      "source": [
        "prediction = []\n",
        "# If hte prediction is > 0.5 then the news is real otherwise it is fake\n",
        "for i in range(len(preds)):\n",
        "    if preds[i].item() > 0.5:\n",
        "        prediction.append(1)\n",
        "    else:\n",
        "        prediction.append(0)\n",
        "\n",
        "# Getting accuracy\n",
        "accuracy = accuracy_score(list(y_test), prediction)\n",
        "print(\"Model accuracy is : \", accuracy)\n",
        "print(classification_report(y_test, prediction))\n",
        "\n",
        "# Confusion matrix\n",
        "c_matrix = confusion_matrix(list(y_test), prediction)\n",
        "plt.figure(figsize=(25,25))\n",
        "sns.heatmap(c_matrix, annot=True)\n",
        "plt.show()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy is :  0.9014416775884666\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90      1905\n",
            "           1       0.90      0.90      0.90      1910\n",
            "\n",
            "    accuracy                           0.90      3815\n",
            "   macro avg       0.90      0.90      0.90      3815\n",
            "weighted avg       0.90      0.90      0.90      3815\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x1800 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FTJW3lx8ojl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}